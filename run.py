# -*- coding: utf-8 -*-
"""gnn_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1I-Iy9Trv5ObZ8rEhP2sarvK1tz0qmUHx
"""



"""* **libraries for single cell preprocessing**"""

!pip install scanpy anndata

import pandas as pd
from scipy.io import mmread

import scipy
import anndata as ad
import pickle
import scanpy as sc

"""* **libraries for lineage coupling analysis**"""

import codecs
from contextlib import closing
import requests
import csv
import itertools
import statistics
import random
import collections
import copy
import math
import pprint as pp
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
import numpy as np
import argparse
import pandas as pd

"""# Prepare for analysis

## preprocess count matrix
"""

# load count matrix
data_path = '/gpfs/slayman/pi/gerstein/db2423/PROJECT/GNN/GSM4185644_stateFate_cytokinePerturbation_normed_counts.mtx.gz'

Matrix = (mmread(data_path))
B = Matrix.todense()
count_matrix = pd.DataFrame(B, range(1, B.shape[0] + 1), range(1, B.shape[1] + 1))
print(count_matrix.shape)

count_matrix.reset_index(inplace=True, drop=True) # reset index with default index

# --------------------
#  Create adata object
# --------------------

# create adata object
exp_X = scipy.sparse.csr_matrix(count_matrix)

# var: gene/features
data_path = '/gpfs/slayman/pi/gerstein/db2423/PROJECT/GNN/GSM4185644_stateFate_cytokinePerturbation_gene_names.txt.gz'
gene_name = pd.read_csv(data_path, sep="\t", header=None)

exp_var = pd.DataFrame(index=gene_name[0]) # Genes/features
exp_var.index.name = 'geneID'

# obs: cell; also incorporate metadata
data_path = '/gpfs/slayman/pi/gerstein/db2423/PROJECT/GNN/GSM4185644_stateFate_cytokinePerturbation_metadata.txt.gz'
metadata = pd.read_csv(data_path, sep="\t", header=0)
metadata.set_index('Cell barcode', inplace=True)

exp_obs = pd.DataFrame({'Time point': metadata["Time point"],
                        'Cell type annotation': metadata["Cell type annotation"],
                        'Cytokine condition': metadata["Cytokine condition"],
                        'SPRING-x': metadata["SPRING-x"],
                        'SPRING-y': metadata["SPRING-y"]}, index=metadata.index)

adata = ad.AnnData(exp_X, obs=exp_obs, var=exp_var)
pickle.dump(adata, open("./adata.pickle", "wb"))

adata = pickle.load(open('./adata.pickle', 'rb'))
adata

# ----------------------------------------
#  Standard preprocessing of mRNA raw data
# ----------------------------------------

adata.obs_names_make_unique()

sc.pp.filter_cells(adata, min_genes=200)
sc.pp.filter_genes(adata, min_cells=20)

adata.var['mt'] = adata.var_names.str.startswith('MT-')  # annotate the group of mitochondrial genes as 'mt'
sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True)

adata = adata[adata.obs.n_genes_by_counts < 2500, :]
adata = adata[adata.obs.pct_counts_mt < 5, :]

sc.pp.normalize_total(adata, target_sum=1e4)

sc.pp.log1p(adata)

sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5)

adata.raw = adata
adata = adata[:, adata.var.highly_variable]

sc.pp.regress_out(adata, ['total_counts', 'pct_counts_mt'])
sc.pp.scale(adata, max_value=10)

adata

# save locally
output_dir = '/gpfs/slayman/pi/gerstein/db2423/PROJECT/GNN/preprocessed_scRNA.h5ad'
adata.write(output_dir)

"""## read in preprocessed data"""

# --------------------------
#  Read in preprocessed data
# --------------------------
data_path = '/gpfs/slayman/pi/gerstein/db2423/PROJECT/GNN/preprocessed_scRNA.h5ad'
adata = ad.read(data_path)

adata

"""## extract clonal info.

**Prepare `processed_clone_matrix` for lineage coupling analysis**: include the following information for each cell:  
* **cellID**: cell identity (barcode);
* **cloneID**: inidicates the clone which the cell belongs to (int);
* **ident**: the cluster/state name which the cell belongs to (str).
"""

# -------------------------------
#  Read in metadata; clone matrix
# -------------------------------
data_path = '/gpfs/slayman/pi/gerstein/db2423/PROJECT/GNN/GSM4185644_stateFate_cytokinePerturbation_metadata.txt.gz'
metadata = pd.read_csv(data_path, sep="\t", header=0)

data_path = '/gpfs/slayman/pi/gerstein/db2423/PROJECT/GNN/GSM4185644_stateFate_cytokinePerturbation_clone_matrix.mtx.gz'
Matrix = (mmread(data_path))
B = Matrix.todense()
clone_matrix = pd.DataFrame(B, range(1, B.shape[0] + 1), range(1, B.shape[1] + 1))

clone_matrix = clone_matrix.T  # cell by clone (one-hot coding)
clone_matrix.reset_index(inplace=True, drop=True) # reset index with default index

print(metadata.shape)
print(sorted(metadata['Time point'].unique()))
print(sorted(metadata['Cell type annotation'].unique()))
print(sorted(metadata['Cytokine condition'].unique()))
metadata.head(5)

clone_matrix.head(5)

print(clone_matrix.shape)

# -------------------------------
#  Preprocessing the clone_matrix
# -------------------------------

# remove clones that have < 5 cells in it
#rows are cells columns are clone id
processed_clone_matrix = clone_matrix.loc[:, clone_matrix.sum(axis=0) >= 5]

# remove cells that don't belong to any clones
processed_clone_matrix = processed_clone_matrix.loc[processed_clone_matrix.sum(axis=1) != 0,:]

# convert 1-hot to cloneID
processed_clone_matrix = processed_clone_matrix.idxmax(1)
processed_clone_matrix = pd.DataFrame({'cloneID':processed_clone_matrix.values}, index=processed_clone_matrix.index)

# --------------------
#  Concat w/ clusterID
# --------------------
i = processed_clone_matrix.index
processed_clone_matrix = pd.concat([processed_clone_matrix, metadata.loc[i, ['Cell barcode', 'Cell type annotation']]], axis=1, ignore_index=False)
processed_clone_matrix.reset_index(drop=True, inplace=True)

processed_clone_matrix.rename(columns = {'Cell barcode':'cellID',
                                         'Cell type annotation':'ident'},
                              inplace = True)

# save processed data locally
output_dir = '/gpfs/slayman/pi/gerstein/db2423/PROJECT/GNN/processed_clone_table.csv.gz'
processed_clone_matrix.to_csv(output_dir, compression='gzip')

print(processed_clone_matrix.shape)
processed_clone_matrix.head()

"""## read in clonal table"""

path = '/gpfs/slayman/pi/gerstein/db2423/PROJECT/GNN/processed_clone_table.csv.gz'
clone_table = pd.read_csv(path,index_col=0, header=0)

clone_table.head(2)

"""# 0-Basic analysis

## cell state visualization
"""

# --------------------------
#  Read in preprocessed data
# --------------------------
data_path = '/gpfs/slayman/pi/gerstein/db2423/PROJECT/GNN/preprocessed_scRNA.h5ad'
adata = ad.read(data_path)

adata

# add SPRING coordinates to adata
adata.obsm["SPRING coordinates"] = pd.concat([adata.obs['SPRING-x'], adata.obs['SPRING-y']], axis=1, ignore_index=False)

adata.obs['Time point'] = adata.obs['Time point'].astype('category')

print(adata.obsm["SPRING coordinates"].head())

print(adata.obsm.keys())

import matplotlib.pyplot as plt
adata.obs['Cell type annotation'] = adata.obs['Cell type annotation'].astype('category')

# Create a scatter plot colored by 'Cell type annotation'
sns.scatterplot(
    x=adata.obsm["SPRING coordinates"]['SPRING-x'],
    y=adata.obsm["SPRING coordinates"]['SPRING-y'],
    hue=adata.obs['Cell type annotation'],
    palette='viridis',  # Or any other palette
    s=20  # Size of the dots
)

plt.title("Cell type annotation")
plt.legend(title='Cell type annotation', fontsize='medium', title_fontsize='small',bbox_to_anchor=(1.05, 1),  # Places the legend to the right of the plot
    loc='upper left')
output_dir = '/gpfs/slayman/pi/gerstein/db2423/PROJECT/GNN/spring_cell_type.png'
plt.savefig(output_dir, dpi=300, bbox_inches="tight", format="png")
plt.show()

import matplotlib.pyplot as plt
adata.obs['Time point'] = adata.obs['Time point'].astype('category')

# Create a scatter plot colored by 'Cell type annotation'
sns.scatterplot(
    x=adata.obsm["SPRING coordinates"]['SPRING-x'],
    y=adata.obsm["SPRING coordinates"]['SPRING-y'],
    hue=adata.obs['Time point'],
    palette='viridis',  # Or any other palette
    s=20  # Size of the dots
)

plt.title("Time point")
plt.legend(title='Time point', fontsize='medium', title_fontsize='small',bbox_to_anchor=(1.05, 1),  # Places the legend to the right of the plot
    loc='upper left')
output_dir = '/gpfs/slayman/pi/gerstein/db2423/PROJECT/GNN/spring_time.png'
plt.savefig(output_dir, dpi=300, bbox_inches="tight", format="png")
plt.show()

"""## identify progenitor cells"""

# read in clone table
path = '/gpfs/slayman/pi/gerstein/db2423/PROJECT/GNN/processed_clone_table.csv.gz'
clone_table = pd.read_csv(path, index_col=0, header=0)

clone_table.head(2)

adata.obs.head(2)

print(clone_table.shape)  # has removed cells that don't belong to any clone
print(adata.obs.shape)

clone_table.set_index('cellID', inplace=True)
clone_table.index.name = 'Cell barcode'

# -------------------------------------
#  Drop duplicated index in clone table
# -------------------------------------

# clone table: additional processing
print((clone_table.index.duplicated()).sum())  # number of duplicated index
clone_table.loc['CCACCTCT-ACGAAACG']  # checked - duplicated rows

clone_table = clone_table[~clone_table.index.duplicated(keep='first')]
clone_table.loc['CCACCTCT-ACGAAACG']

# --------------------------
#  Add clonal info. to adata
# --------------------------
# remove the index that is in clone_table, but not in adata
idx_not_in_adata = [i for i in clone_table.index if i not in adata.obs.index]
i = pd.Index(idx_not_in_adata)
idx_to_assign_clone = clone_table.index.drop(i)

idx_assign_neg = [i for i in adata.obs.index if i not in idx_to_assign_clone]
idx_assign_neg = pd.Index(idx_assign_neg)

c1 = clone_table.loc[idx_to_assign_clone, 'cloneID'].to_frame()
c2 = pd.DataFrame({'cloneID': -1}, index=idx_assign_neg).astype('int64')
c2.index.name = 'Cell barcode'

pd.concat([c1, c2]).head(2)

x_test=pd.concat([c1,c2])
x_test.index

adata.obs.index

index1 = pd.Index(x_test.index)
index2 = pd.Index(adata.obs.index)
index1.equals(index2)

x_test_sorted = x_test.reindex(adata.obs.index)
index1 = pd.Index(x_test_sorted.index)
index2 = pd.Index(adata.obs.index)
index1.equals(index2)

# add cloneID into obs
adata.obs['cloneID'] = x_test_sorted['cloneID']
adata.obs['cloneID']

"""**in cloneID, -1 means the cell doesn't belong to any clone**"""

adata.obs['Cell type annotation'].unique()

# ----------------------------------------------------------
#  Add new attributes of progenitor cells for each cell type
# ----------------------------------------------------------
def add_progenitor_cells(cell_type_oi):
    # list of cloneID of specific cell type of interest
    cell_type = (adata.obs['Cell type annotation'] == cell_type_oi)
    cloneID_list = list(adata.obs.loc[cell_type, 'cloneID'].unique())

    is_undifferentiated = (adata.obs['Cell type annotation'] == 'Undifferentiated')
    has_clone = adata.obs['cloneID'] != -1
    is_mo_clone = adata.obs['cloneID'].isin(cloneID_list)

    # add new columns: flag indicative of Mo progenitor cells
    adata.obs[cell_type_oi + '_progenitor_cells'] = is_undifferentiated & has_clone & is_mo_clone
    return adata.obs[cell_type_oi + '_progenitor_cells'].sum()

for coi in adata.obs['Cell type annotation'].unique():
    if coi == 'Undifferentiated':
        continue
    print(coi + ':')
    print(add_progenitor_cells(coi)) # count of progenitor

adata.obs.head(2)

"""# 1-Hierarchical clustering on clonal state"""

# --------------------------------------
#  Additinoal processing of clonal table
# --------------------------------------
# read in clone table
path = '/gpfs/slayman/pi/gerstein/db2423/PROJECT/GNN/processed_clone_table.csv.gz'
clone_table = pd.read_csv(path, index_col=0, header=0)

clone_table.head(2)

print(clone_table.shape)  # has removed cells that don't belong to any clone

clone_table.set_index('cellID', inplace=True)
clone_table.index.name = 'Cell barcode'

# drop duplicated index in clone table
print((clone_table.index.duplicated()).sum())  # number of duplicated index
clone_table.loc['CCACCTCT-ACGAAACG']  # checked - duplicated rows

clone_table = clone_table[~clone_table.index.duplicated(keep='first')]
clone_table.loc['CCACCTCT-ACGAAACG']

# convert index back to a column; use default index
clone_table = clone_table.reset_index()

clone_table.rename(columns = {'Cell barcode':'cellID'},
                              inplace = True)

clone_table.head(2)

# ----------------------------------
#  Drop undifferentiatied cell types
# ----------------------------------
undifferentiated = clone_table[clone_table['ident'] == 'Undifferentiated']
clone_table.drop(undifferentiated.index, inplace=True)

# save processed data locally
output_dir = '/gpfs/slayman/pi/gerstein/db2423/PROJECT/GNN/final_clone_table.csv.gz'
clone_table.to_csv(output_dir, compression='gzip')

# --------------------------------------------
#  Declare parameters of lineage coupling
# --------------------------------------------
csv_data_file = '/gpfs/slayman/pi/gerstein/db2423/PROJECT/GNN/final_clone_table.csv'

csv_data_url = None
vmin = -20.0
vmax = 20.0
num_shufflings = 10000

# output dir
csv_metric_values_matrix_output_file = (
                '/gpfs/slayman/pi/gerstein/db2423/PROJECT/GNN/metric_values_per_state_pair_matrix.csv')
csv_lineage_coupling_scores_matrix_output_file = (
                '/gpfs/slayman/pi/gerstein/db2423/PROJECT/GNN/lineage_coupling_scores_matrix.csv')

# csv_data_url = ('https://keeper.mpdl.mpg.de/lib'
#                 '/011bef7d-af64-4883-9135-afebe1e25e1e/file/results/TIJ'
#                 '/results/lb_pool/TIJ_HARMONY_Clusters.csv?dl=1')

# The url is given priority as the data source over the local file
csv_data_source_str = (csv_data_url
                            if (csv_data_url is not None)
                            else csv_data_file)

"""## compute clonal distance"""

print("Run 'python lineage_coupling_analysis_wagner_way_5.py -h' for displaying usage.")
# Iterable that stores the parsed data
csv_data_clone_cells = parse_csv_data(csv_data_url, csv_data_file)
# Counter of cells per clone
clones = compute_clones_distribution(csv_data_clone_cells)

# --------------------------------------------
#  Compute metric values per state pair
# --------------------------------------------

# Counter of cells per cluster, and names of each cluster
clusters, clusters_names = compute_clusters_distribution(csv_data_clone_cells)
print(clusters)
print("1")
print(clusters_names)
# Dictionary-valued dictionary indexed by the cluster pairs.
cluster_pairs = generate_cluster_pairs(clusters.keys())

# Multi-level counter with different cell counts per level
num_cells = compute_num_cells_per_clones_per_cluster(csv_data_clone_cells)
cluster_pairs = compute_metric_values_all_pairs(
                                        num_cells, cluster_pairs, clones) #computes metric values for each pair of clusters, using the number of cells per clone per cluster
print("Writing metric values per pair into csv file '{}'"
        .format(csv_metric_values_matrix_output_file))
write_matrix("Matrix of metric values per pair"
             , "metric_value", cluster_pairs
             , clusters, clusters_names
             , csv_metric_values_matrix_output_file)

print("Computing lineage coupling scores for data in '{}'"
    ", with {} shufflings, clustermap_minimum {}, and clustermap_maximum {}"
    .format(csv_data_source_str, num_shufflings, vmin, vmax))
cluster_pairs = compute_distributions(cluster_pairs, num_shufflings
                    , csv_data_clone_cells, clusters)

# --------------------------------------------
#  Compute z-score per state pair
# --------------------------------------------
cluster_pairs = compute_z_scores(cluster_pairs)
pp.pprint(cluster_pairs)

"""## hierarchical clustering

**Need to declare the following parameters:**
* vmin/vmax;
* output_dir x2 for w/, w/o annotation.
"""

# --------------------------------------------
#  Hierarchical clustering
# --------------------------------------------
# may need to adjust the following parameters for a better viz
vmin = -10.0
vmax = 10.0

sns.set(font_scale=1.1)
output_dir = path = '/gpfs/slayman/pi/gerstein/db2423/PROJECT/GNN/hc_clone_dist_wo_annot.png'

output_lineage_coupling_computations(
    cluster_pairs, clusters, clusters_names
    , csv_lineage_coupling_scores_matrix_output_file
    , vmin, vmax
    , output_dir
    , add_annot=False)

# w/ annotation on heatmap
output_dir = path = '/gpfs/slayman/pi/gerstein/db2423/PROJECT/GNN/hc_clone_dist_w_annot.png'
output_lineage_coupling_computations(
    cluster_pairs, clusters, clusters_names
    , csv_lineage_coupling_scores_matrix_output_file
    , vmin, vmax
    , output_dir
    , add_annot=True)

path="/gpfs/slayman/pi/gerstein/db2423/PROJECT/GNN/lineage_coupling_scores_matrix.csv"
clone_distance_matrix = pd.read_csv(path, index_col=0, header=0)

clone_distance_matrix.head(5)

"""# 2-Hierarchical clustering on cell state

## 2.0 define cell state
"""

adata

# load adata
data_path = '/gpfs/slayman/pi/gerstein/db2423/PROJECT/GNN/preprocessed_scRNA.h5ad'
adata = ad.read(data_path)

# remove undifferentiated cells
adata = adata[~adata.obs['Cell type annotation'].isin(['Undifferentiated']),:]  # create a view, not a copy

# avg of expression features over all cell types
res = pd.DataFrame(columns=adata.var_names, index=adata.obs['Cell type annotation'].cat.categories) # result df
for c in adata.obs['Cell type annotation'].cat.categories:
    res.loc[c] = adata[adata.obs['Cell type annotation'].isin([c]),:].X.mean(0)

res

"""## 2.1 Euclidean distance"""

# --------------------------------------
#  Gaussian kernel; adaptative bandwidth
# --------------------------------------
from scipy.spatial.distance import squareform
from scipy.spatial.distance import pdist
res_numeric = res.apply(pd.to_numeric, errors='coerce')
# distance matrix based on cell state
D = compute_distances(res_numeric.values)  # matrix: cell type x (avg) gene
A = compute_affinity_matrix(D, kernel_type="adaptive", k=5, sigma=None)

D_df = pd.DataFrame(D, index=res.index, columns=res.index)
A_df = pd.DataFrame(A, index=res.index, columns=res.index)

D_df.head(5)

A_df.head(5)

"""## 2.2 diffusion distance"""

t = 5
import scipy.linalg as la
diff_vec, diff_eig = diff_map_info(A)
diff_map = get_diff_map(diff_vec, diff_eig, t=t)

diff_coords = diff_map[:,0:3]  # [cell type, coords (take first 3 coordinates here)]

diff_D = compute_distances(diff_coords)  # diffusion distance
diff_D = pd.DataFrame(diff_D, index=res.index, columns=res.index)

"""## 2.2 HC based on various distance"""

# Euclidean distance
vmin=0
vmax=50
import scipy.spatial.distance as distance
import scipy.cluster.hierarchy as hc
desired_order = ['Mo', 'Neu', 'Ma', 'pDC1', 'cDC', 'Eos', 'pDC2', 'Ba', 'Mk', 'Er']

# Reindex the DataFrame according to the desired order for both rows and columns
D_df_ordered = D_df.reindex(index=desired_order, columns=desired_order)

# Plot the clustermap with the ordered DataFrame without hierarchical linkage
with sns.axes_style("white"):
    ax = sns.clustermap(D_df_ordered, cmap="Blues", vmin=vmin, vmax=vmax)
    plt.setp(ax.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)
    plt.setp(ax.ax_heatmap.xaxis.get_majorticklabels(), rotation=90)  # Rotate x-axis labels if needed

plt.show()

output_dir = '/gpfs/slayman/pi/gerstein/db2423/PROJECT/GNN/hc_diffusion_dist.png'

# diffusion distance
vmin=0
vmax=0.0015

squareform_distance = distance.squareform(diff_D.values)

# Perform hierarchical clustering
linkage = hc.linkage(squareform_distance, method='average')
with sns.axes_style("white"):
    ax = sns.clustermap(diff_D, row_linkage=linkage, col_linkage=linkage, cmap="Blues",
                        vmin=vmin, vmax=vmax)
    plt.setp(ax.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)
plt.savefig(output_dir, dpi=300, bbox_inches="tight", format="png")
plt.show()

clone_distance_matrix

path="/gpfs/slayman/pi/gerstein/db2423/PROJECT/GNN/diff_distance_matrix"
diff_D.to_csv(path)
diff_D_distance = pd.read_csv(path, index_col=0, header=0)

diff_D_distance.head(5)

#Construct GNN

def z_scores_to_scaled_affinity(z_scores):
    # Apply exponential function to positive z-scores
    positive_affinities = np.exp(z_scores)
    positive_affinities[z_scores < 0] = 0

    # Apply scaled exponential function to negative z-scores
    # The scaling factor (e.g., 0.8) reduces the impact of negative z-scores
    negative_affinities = np.exp(-abs(z_scores)) * 0.8
    negative_affinities[z_scores >= 0] = 0

    # Combine positive and negative affinities
    affinities = positive_affinities + negative_affinities

    # Normalize the affinities so that they sum to 1 for each node
    affinity_sums = affinities.sum(axis=1)
    normalized_affinities = affinities / affinity_sums

    # Set the diagonal to zero if you don't want self-loops in the GNN
    np.fill_diagonal(normalized_affinities, 0)

    return normalized_affinities

def diffusion_to_affinity(diffusion_distance):
    # Apply an exponential decay to convert distances to affinities
    affinities = np.exp(-diffusion_distance)

    # Set the diagonal to zero if you don't want self-loops in the GNN
    np.fill_diagonal(affinities, 0)

    # Normalize the affinities so that they sum to 1 for each node
    affinity_sums = affinities.sum(axis=1)
    normalized_affinities = affinities / affinity_sums

    return normalized_affinities

clonal_affinity = z_scores_to_scaled_affinity(clone_distance_matrix.values)
diffusion_affinity = diffusion_to_affinity(diff_D_distance.values)

# create Edge Index and Edge Weight Tensors

pip install torch

pip install torch-geometric

import torch
from torch_geometric.data import Data

def create_edge_index_and_weights(affinity_matrix):
    edge_index = []
    edge_weight = []

    for i, row in enumerate(affinity_matrix):
        for j, weight in enumerate(row):
            if i != j and weight > 0:  # Assuming no self-loops and positive weights
                edge_index.append((i, j))
                edge_weight.append(weight)

    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()
    edge_weight = torch.tensor(edge_weight, dtype=torch.float)

    return edge_index, edge_weight

# Assume clonal_affinity and diffusion_affinity are numpy arrays from your affinity matrices
edge_index_clonal, edge_weight_clonal = create_edge_index_and_weights(clonal_affinity)
edge_index_diffusion, edge_weight_diffusion = create_edge_index_and_weights(diffusion_affinity)

import torch
import torch.nn.functional as F
from torch_geometric.nn import GCNConv

class GNNModel(torch.nn.Module):
    def __init__(self, input_dim, hidden_dim):
        super(GNNModel, self).__init__()
        self.conv1 = GCNConv(input_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, hidden_dim)

    def forward(self, x, edge_index, edge_weight):
        x = self.conv1(x, edge_index, edge_weight=edge_weight)
        x = F.relu(x)
        x = self.conv2(x, edge_index, edge_weight=edge_weight)
        return x  # Return embeddings

# begin diffusion distance edge weight first

from torch_geometric.data import Data

num_states = 10  # Adjust based on your data
node_features = torch.eye(num_states)  # One-hot encoding

# Assuming edge_index_clonal and edge_weight_clonal are defined
data_diffusion = Data(x=node_features, edge_index=edge_index_diffusion, edge_attr=edge_weight_diffusion)

edge_index_diffusion

from torch_geometric.utils import to_dense_adj


num_states = 10  # Adjust based on your data
node_features = torch.eye(num_states)  # One-hot encoding
num_epochs = 200
num_runs = 3
final_losses = []

def train(model, data, optimizer):
    model.train()
    optimizer.zero_grad()
    embeddings = model(data.x, data.edge_index, data.edge_attr)
    reconstructed_adj = torch.sigmoid(torch.mm(embeddings, embeddings.t()))
    loss = F.mse_loss(reconstructed_adj, original_adj)
    loss.backward()
    optimizer.step()
    return loss.item()

for run in range(num_runs):
    # Reinitialize model and optimizer
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = GNNModel(num_states, 64).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    data = data_diffusion.to(device)
    original_adj = to_dense_adj(data.edge_index, edge_attr=data.edge_attr)[0].to(device)

    for epoch in range(num_epochs):
        loss = train(model, data, optimizer)
        if epoch == num_epochs - 1:
            final_losses.append(loss)

# Calculate mean and standard deviation of losses
mean_loss = np.mean(final_losses)
std_loss = np.std(final_losses)

# Display the results in a table format
print(f"Run\tFinal Loss")
for i, loss in enumerate(final_losses):
    print(f"{i+1}\t{loss:.4f}")
print(f"Mean Loss: {mean_loss:.4f}")
print(f"Std Deviation of Loss: {std_loss:.4f}")

model.eval()
embeddings = model(data.x, data.edge_index, data.edge_attr).detach().cpu().numpy()
n_samples = embeddings.shape[0]

tsne_perplexity = min(30, n_samples - 1)
# t-SNE for visualization
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

tsne = TSNE(n_components=2, perplexity=tsne_perplexity, random_state=42)
embeddings_2d = tsne.fit_transform(embeddings)


labels = ['Ba', 'Eos', 'Er', 'Ma', 'Mk', 'Mo', 'Neu', 'cDC', 'pDC1', 'pDC2']

plt.figure(figsize=(10, 8), dpi=300)
for i, label in enumerate(labels):
    plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1], label=label)

plt.title("Node Embedding of GNN With Diffusion Distance as Edge Weight")
plt.xlabel("t-SNE Component 1")
plt.ylabel("t-SNE Component 2")

# Annotate the points with labels
for i, label in enumerate(labels):
    plt.annotate(label, (embeddings_2d[i, 0], embeddings_2d[i, 1]), textcoords="offset points", xytext=(10,-10), ha='center')

plt.legend(loc='upper left', bbox_to_anchor=(1, 1))

# Save the figure
plt.savefig('/gpfs/slayman/pi/gerstein/db2423/PROJECT/GNN/tSNE_Node_Embedding_diffusion_distance.png', bbox_inches='tight')

plt.show()

from scipy.cluster.hierarchy import dendrogram, linkage
from matplotlib import pyplot as plt

linked = linkage(embeddings, 'average')
plt.figure(figsize=(10, 7),dpi=300)
dendrogram(linked, labels=labels, orientation='top')
plt.title('Hierarchical Clustering Dendrogram of Node Embedding of GNN With Diffusion Distance as Edge Weight')
plt.savefig('/gpfs/slayman/pi/gerstein/db2423/PROJECT/GNN/hierarchical_Node_Embedding_diffusion_distance_gnn.png', bbox_inches='tight')
plt.show()

np.shape(edge_index_diffusion)

np.shape(edge_weight_diffusion)

num_states = 10  # Number of nodes
node_features = torch.eye(num_states)  # One-hot encoding of node features

# Initialize the edge weight matrix with zeros
edge_weight_matrix = torch.zeros((num_states, num_states), dtype=torch.float)

# Fill in the edge weights based on the edge_index_diffusion and edge_weight_diffusion
for i in range(edge_index_diffusion.size(1)):
    src, dest = edge_index_diffusion[:, i].tolist()
    weight = edge_weight_diffusion[i].item()
    edge_weight_matrix[src, dest] = weight

# Since we don't want self-loops, we can ensure the diagonal is zero.
# This is optional because the diagonal of node_features is already 1 and won't be affected by adding zeros.
for i in range(num_states):
    edge_weight_matrix[i, i] = 0

# Augment node features by concatenating with the edge weight matrix
augmented_node_features = torch.cat([node_features, edge_weight_matrix], dim=1)

# Create the PyG Data object with the augmented node features
data = Data(x=augmented_node_features, edge_index=edge_index_diffusion)

from torch_geometric.nn import GATConv

# Define the GAT model
class GATModel(torch.nn.Module):
    def __init__(self, input_dim, hidden_dim, heads=1):
        super(GATModel, self).__init__()
        self.conv1 = GATConv(input_dim, hidden_dim, heads=heads, concat=True)
        self.conv2 = GATConv(hidden_dim * heads, hidden_dim, heads=1, concat=False)

    def forward(self, x, edge_index):
        x = F.dropout(x, p=0.6, training=self.training)
        x = F.elu(self.conv1(x, edge_index))
        x = F.dropout(x, p=0.6, training=self.training)
        x = self.conv2(x, edge_index)
        return x

num_states = 10  # Adjust based on your data
node_features = torch.eye(num_states)  # One-hot encoding
num_epochs = 200
num_runs = 3
final_losses = []
# Define the training loop
def train(model,data,optimizer):
    model.train()
    optimizer.zero_grad()
    out = model(data.x, data.edge_index)
    # Since we don't have labels, we might use unsupervised loss here; e.g., reconstructing the diffusion matrix
    reconstructed_diffusion_matrix = torch.sigmoid(torch.mm(out, out.t()))
    mask = torch.ones_like(reconstructed_diffusion_matrix) - torch.eye(num_states, device=device)
    reconstructed_diffusion_matrix_off_diag = reconstructed_diffusion_matrix * mask
    edge_weight_matrix_off_diag = edge_weight_matrix * mask
    loss = F.mse_loss(reconstructed_diffusion_matrix_off_diag, edge_weight_matrix_off_diag)
    loss.backward()
    optimizer.step()
    return loss.item()

for run in range(num_runs):
    # Reinitialize model and optimizer
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    input_dim = augmented_node_features.shape[1]
    model = GATModel(input_dim=input_dim, hidden_dim=64, heads=8).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    data = data.to(device)

    for epoch in range(num_epochs):
        loss = train(model, data, optimizer)
        if epoch == num_epochs - 1:
            final_losses.append(loss)

# Calculate mean and standard deviation of losses
mean_loss = np.mean(final_losses)
std_loss = np.std(final_losses)

# Display the results in a table format
print(f"Run\tFinal Loss")
for i, loss in enumerate(final_losses):
    print(f"{i+1}\t{loss:.4f}")
print(f"Mean Loss: {mean_loss:.4f}")
print(f"Std Deviation of Loss: {std_loss:.4f}")

model.eval()
embeddings = model(data.x, data.edge_index).detach().cpu().numpy()
n_samples = embeddings.shape[0]

tsne_perplexity = min(30, n_samples - 1)
# t-SNE for visualization
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

tsne = TSNE(n_components=2, perplexity=tsne_perplexity, random_state=42)
embeddings_2d = tsne.fit_transform(embeddings)


labels = ['Ba', 'Eos', 'Er', 'Ma', 'Mk', 'Mo', 'Neu', 'cDC', 'pDC1', 'pDC2']

plt.figure(figsize=(10, 8), dpi=300)
for i, label in enumerate(labels):
    plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1], label=label)

plt.title("Node Embedding of GAT With Diffusion Distance as Edge Weight")
plt.xlabel("t-SNE Component 1")
plt.ylabel("t-SNE Component 2")

# Annotate the points with labels
for i, label in enumerate(labels):
    plt.annotate(label, (embeddings_2d[i, 0], embeddings_2d[i, 1]), textcoords="offset points", xytext=(10,-10), ha='center')

plt.legend(loc='upper left', bbox_to_anchor=(1, 1))

# Save the figure
plt.savefig('/gpfs/slayman/pi/gerstein/db2423/PROJECT/GNN/tSNE_Node_Embedding_diffusion_distance_gat.png', bbox_inches='tight')

plt.show()

from scipy.cluster.hierarchy import dendrogram, linkage
from matplotlib import pyplot as plt

linked = linkage(embeddings, 'average')
plt.figure(figsize=(10, 7),dpi=300)
dendrogram(linked, labels=labels, orientation='top')
plt.title('Hierarchical Clustering Dendrogram of Node Embedding of GAT With Diffusion Distance as Edge Weight')
plt.savefig('/gpfs/slayman/pi/gerstein/db2423/PROJECT/GNN/hierarchical_Node_Embedding_diffusion_distance_gat.png', bbox_inches='tight')
plt.show()

# clonal distance gnn

from torch_geometric.data import Data

num_states = 10  # Adjust based on your data
node_features = torch.eye(num_states)  # One-hot encoding

# Assuming edge_index_clonal and edge_weight_clonal are defined
data_clonal= Data(x=node_features, edge_index=edge_index_clonal, edge_attr=edge_weight_clonal)

from torch_geometric.utils import to_dense_adj


num_states = 10  # Adjust based on your data
node_features = torch.eye(num_states)  # One-hot encoding
num_epochs = 200
num_runs = 3
final_losses = []

def train(model, data, optimizer):
    model.train()
    optimizer.zero_grad()
    embeddings = model(data.x, data.edge_index, data.edge_attr)
    reconstructed_adj = torch.sigmoid(torch.mm(embeddings, embeddings.t()))
    loss = F.mse_loss(reconstructed_adj, original_adj)
    loss.backward()
    optimizer.step()
    return loss.item()

for run in range(num_runs):
    # Reinitialize model and optimizer
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = GNNModel(num_states, 64).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    data = data_clonal.to(device)
    original_adj = to_dense_adj(data.edge_index, edge_attr=data.edge_attr)[0].to(device)

    for epoch in range(num_epochs):
        loss = train(model, data, optimizer)
        if epoch == num_epochs - 1:
            final_losses.append(loss)

# Calculate mean and standard deviation of losses
mean_loss = np.mean(final_losses)
std_loss = np.std(final_losses)

# Display the results in a table format
print(f"Run\tFinal Loss")
for i, loss in enumerate(final_losses):
    print(f"{i+1}\t{loss:.4f}")
print(f"Mean Loss: {mean_loss:.4f}")
print(f"Std Deviation of Loss: {std_loss:.4f}")

# clonal distance gat
num_states = 10  # Number of nodes
node_features = torch.eye(num_states)  # One-hot encoding of node features

# Initialize the edge weight matrix with zeros
edge_weight_matrix = torch.zeros((num_states, num_states), dtype=torch.float)

# Fill in the edge weights based on the edge_index_diffusion and edge_weight_diffusion
for i in range(edge_index_clonal.size(1)):
    src, dest = edge_index_clonal[:, i].tolist()
    weight = edge_weight_clonal[i].item()
    edge_weight_matrix[src, dest] = weight

# Since we don't want self-loops, we can ensure the diagonal is zero.
# This is optional because the diagonal of node_features is already 1 and won't be affected by adding zeros.
for i in range(num_states):
    edge_weight_matrix[i, i] = 0

# Augment node features by concatenating with the edge weight matrix
augmented_node_features = torch.cat([node_features, edge_weight_matrix], dim=1)

# Create the PyG Data object with the augmented node features
data = Data(x=augmented_node_features, edge_index=edge_index_clonal)

from torch_geometric.nn import GATConv

# Define the GAT model
class GATModel(torch.nn.Module):
    def __init__(self, input_dim, hidden_dim, heads=1):
        super(GATModel, self).__init__()
        self.conv1 = GATConv(input_dim, hidden_dim, heads=heads, concat=True)
        self.conv2 = GATConv(hidden_dim * heads, hidden_dim, heads=1, concat=False)

    def forward(self, x, edge_index):
        x = F.dropout(x, p=0.6, training=self.training)
        x = F.elu(self.conv1(x, edge_index))
        x = F.dropout(x, p=0.6, training=self.training)
        x = self.conv2(x, edge_index)
        return x

num_states = 10  # Adjust based on your data
node_features = torch.eye(num_states)  # One-hot encoding
num_epochs = 200
num_runs = 3
final_losses = []
# Define the training loop
def train(model,data,optimizer):
    model.train()
    optimizer.zero_grad()
    out = model(data.x, data.edge_index)
    # Since we don't have labels, we might use unsupervised loss here; e.g., reconstructing the diffusion matrix
    reconstructed_clonal_matrix = torch.sigmoid(torch.mm(out, out.t()))
    mask = torch.ones_like(reconstructed_clonal_matrix) - torch.eye(num_states, device=device)
    reconstructed_clonal_matrix_off_diag = reconstructed_clonal_matrix * mask
    edge_weight_matrix_off_diag = edge_weight_matrix * mask
    loss = F.mse_loss(reconstructed_clonal_matrix_off_diag, edge_weight_matrix_off_diag)
    loss.backward()
    optimizer.step()
    return loss.item()

for run in range(num_runs):
    # Reinitialize model and optimizer
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    input_dim = augmented_node_features.shape[1]
    model = GATModel(input_dim=input_dim, hidden_dim=64, heads=8).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    data = data.to(device)

    for epoch in range(num_epochs):
        loss = train(model, data, optimizer)
        if epoch == num_epochs - 1:
            final_losses.append(loss)

# Calculate mean and standard deviation of losses
mean_loss = np.mean(final_losses)
std_loss = np.std(final_losses)

# Display the results in a table format
print(f"Run\tFinal Loss")
for i, loss in enumerate(final_losses):
    print(f"{i+1}\t{loss:.4f}")
print(f"Mean Loss: {mean_loss:.4f}")
print(f"Std Deviation of Loss: {std_loss:.4f}")

model.eval()
embeddings_clonal = model(data.x, data.edge_index).detach().cpu().numpy()
linked = linkage(embeddings_clonal, 'average')
plt.figure(figsize=(10, 7),dpi=300)
labels = ['Ba', 'Ma', 'Mo', 'Neu', 'pDC1', 'cDC', 'Mk', 'Eos', 'Er', 'pDC2']
dendrogram(linked, labels=labels, orientation='top')
plt.title('Hierarchical Clustering Dendrogram of Node Embedding of GAT With Clonal Distance as Edge Weight')
plt.savefig('/gpfs/slayman/pi/gerstein/db2423/PROJECT/GNN/hierarchical_Node_Embedding_clonal_distance_gat.png', bbox_inches='tight')
plt.show()

distances = pdist(embeddings_clonal, metric='euclidean')

# Convert to a square form distance matrix
distance_matrix = squareform(distances)

# Apply an exponential decay to the distance matrix to get similarity
# Adjust the scale of the decay to your preference
decay_factor = 1.0  # This is an arbitrary scale factor for the decay
ajacency_matrix = np.exp(-decay_factor * distance_matrix)


# Create a graph from the adjacency matrix

G = nx.from_numpy_array(adjacency_matrix)
isolated_nodes = list(nx.isolates(G))

# If you have isolated nodes, you might choose to connect them with a small value
# or handle them in a domain-specific manner
for node in isolated_nodes:
    # Connect isolated nodes with a small value or handle otherwise
    # Example: Connect all isolated nodes to node 0 with a small similarity value
    G.add_edge(node, 0, weight=0)

# Calculate the clustering coefficient for the entire network
clustering_coefficients = nx.clustering(G)
average_clustering_coefficient = nx.average_clustering(G)

# Calculate the shortest path lengths for all pairs of nodes
shortest_path_lengths = dict(nx.floyd_warshall(G))

shortest_path_lengths

clustering_coeffs.values()

average_clustering_coefficient

labels = ['Ba', 'Ma', 'Mo', 'Neu', 'pDC1', 'cDC', 'Mk', 'Eos', 'Er', 'pDC2']
# Create a DataFrame for plotting
df = pd.DataFrame({
    'Clustering Coefficient': clustering_coeffs.values(),
    'Cell States': labels
})

# Set the plot size
plt.figure(figsize=(10, 8))

# Create the bar plot
sns.barplot(x='Cell States', y='Clustering Coefficient', data=df, palette="YlGnBu")

# Set the title and labels
plt.title('Clustering Coefficients Constructed with Clonal Distance')
plt.xlabel('Labels')
plt.ylabel('Clustering Coefficient')

# Save the figure with a DPI of 300
plt.savefig('/gpfs/slayman/pi/gerstein/db2423/PROJECT/GNN/clustering_coefficients_clonal_distance.png', dpi=300)

# Show the plot
plt.show()

# The labels for each coefficient
path_lengths_df = pd.DataFrame(shortest_path_lengths)

# Define the labels for the axes

# Plot the heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(path_lengths_df, cmap="viridis", xticklabels=labels, yticklabels=labels)

# Add the title
plt.title("Shortest Path Lengths Constructed with Clonal Distance")

# Save the figure with a DPI of 300
plt.savefig('/gpfs/slayman/pi/gerstein/db2423/PROJECT/GNN/shortest_path_lengths_heatmap_clonal.png', dpi=300)

# Display the plot
plt.show()

#diffusion distance
distances = pdist(embeddings, metric='euclidean')

# Convert to a square form distance matrix
distance_matrix = squareform(distances)

# Apply an exponential decay to the distance matrix to get similarity
# Adjust the scale of the decay to your preference
decay_factor = 1.0  # This is an arbitrary scale factor for the decay
ajacency_matrix = np.exp(-decay_factor * distance_matrix)


# Create a graph from the adjacency matrix

G = nx.from_numpy_array(adjacency_matrix)
isolated_nodes = list(nx.isolates(G))

# If you have isolated nodes, you might choose to connect them with a small value
# or handle them in a domain-specific manner
for node in isolated_nodes:
    # Connect isolated nodes with a small value or handle otherwise
    # Example: Connect all isolated nodes to node 0 with a small similarity value
    G.add_edge(node, 0, weight=0)

# Calculate the clustering coefficient for the entire network
clustering_coefficients = nx.clustering(G)
average_clustering_coefficient = nx.average_clustering(G)

# Calculate the shortest path lengths for all pairs of nodes
shortest_path_lengths = dict(nx.floyd_warshall(G))

# Create a DataFrame for plotting
labels = ['Ba', 'Eos', 'Er', 'Ma', 'Mk', 'Mo', 'Neu', 'cDC', 'pDC1', 'pDC2']
df = pd.DataFrame({
    'Clustering Coefficient': clustering_coeffs.values(),
    'Cell States': labels
})

# Set the plot size
plt.figure(figsize=(10, 8))

# Create the bar plot
sns.barplot(x='Cell States', y='Clustering Coefficient', data=df, palette="YlGnBu")

# Set the title and labels
plt.title('Clustering Coefficients Constructed with Diffusion Distance')
plt.xlabel('Labels')
plt.ylabel('Clustering Coefficient')

# Save the figure with a DPI of 300
plt.savefig('/gpfs/slayman/pi/gerstein/db2423/PROJECT/GNN/clustering_coefficients_diffusion_distance.png', dpi=300)

# Show the plot
plt.show()

path_lengths_df = pd.DataFrame(shortest_path_lengths)

# Define the labels for the axes

# Plot the heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(path_lengths_df, cmap="viridis", xticklabels=labels, yticklabels=labels)

# Add the title
plt.title("Shortest Path Lengths Constructed with Diffusion Distance")

# Save the figure with a DPI of 300
plt.savefig('/gpfs/slayman/pi/gerstein/db2423/PROJECT/GNN/shortest_path_lengths_heatmap_diffusion.png', dpi=300)

# Display the plot
plt.show()

"""## Helper function"""









def extract_rows(csv_reader, csv_data_clone_cells):
    """
    Extract relevant row values from a csv.DictReader() object.

    Obs:
        In some datasets there may be a column, 'ident_name'
        which stores the names of the cell states.
        That column is used for parts that should output something
        about the cell states (e.g. metric values in the csv).
        However, in all datasets used for the analyses in the paper,
        there wasn't such column, so the column 'ident' itself is used
        as the column name.
    """
    row_tmp = {}
    for i, row in enumerate(csv_reader):
        # Store the register in a new dedicated list.
        # Discard all registers that had an empty value
        # for any of the fields
        # Also, for some reason, if cloneID is converted into a string
        # rather than a integer, then other, unrelated
        # floating computations become a little bit unstable
        # (e.g. the metric values begin to differ in
        # their latter decimals when computed with
        # the exact same parameters)
        row_tmp["cloneID"] = int(row["cloneID"])
        if(row_tmp["cloneID"] == ""):
            print("Register number {}.".format(i))
            print("cloneID is empty."
                    .format(row["cloneID"])
                    )
            continue

        row_tmp["ident_name"] = str(row["ident"])
        if(row_tmp["ident_name"] == ""):
            print("Register number {}.".format(i))
            print("ident_name is empty.\n"
                    .format(row["ident"])
                    )
            continue

        row_tmp["ident"] = str(row["ident"])
        if(row_tmp["ident"] == ""):
            print("Register number {}.".format(i))
            print("ident is empty.\n"
                .format(row["ident"])
                )
            continue

        csv_data_clone_cells.append(row_tmp.copy())
    return csv_data_clone_cells

def parse_csv_data(csv_data_url, csv_data_file):
    """
    Return the relevant parsed data from a selected source.

    The relevant data are constituted by:
        1) Data of cells that belong to a clone,
        without including the name of the cluster.
        2) The names of each cluster.
    """
    csv_data_clone_cells = []
    # Before calling this function, if the selected source was
    # a csv file, then csv_data_url was set to None. Otherwise,
    # the url source has priority.
    if(csv_data_url is not None):
        response = requests.get(csv_data_url)
        with closing(requests.get(csv_data_url, stream=True)) as req:
            csv_reader = csv.DictReader(codecs.iterdecode(
                                                req.iter_lines(), 'utf-8'))
            csv_data_clone_cells = extract_rows(csv_reader
                                                    , csv_data_clone_cells)
    else:
        with open(csv_data_file, newline='') as csvfile:   ###
            csv_reader = csv.DictReader(csvfile)
            csv_data_clone_cells = extract_rows(csv_reader
                                                    , csv_data_clone_cells)
            csvfile.close()
    return csv_data_clone_cells

def compute_clones_distribution(csv_data_clone_cells):
    """Return a counter of cells per clone."""
    clones = collections.Counter()
    for row in csv_data_clone_cells:
        cloneID = row["cloneID"]
        clones[cloneID] += 1
    return clones

def compute_clusters_distribution(csv_data_clone_cells):
    """Return a counter of cells per cluster, and their names."""
    clusters = collections.Counter()
    clusters_names = {}
    for row in csv_data_clone_cells:
        clusterID = row["ident"]
        cluster_name = row["ident_name"]
        clusters[clusterID] += 1
        clusters_names[clusterID] = cluster_name
    return clusters, clusters_names

def generate_cluster_pairs(clusters_list):
    """
    Return a dictionary-valued dictionary, indexed by the cluster pairs.

    The indices are 2-tuples, whose elements are the clusterIDs.
    Despite this, each pair is conceptually considered to be
    an unordered pair, and hereby appearing only once.
    This dictionary will later store all relevant data
    of each pair of clusters:
        - The metric values for the clusters
        (according to the definition given
        in the Methods section of the paper),
        resulting from the data in the dataset.
        - The parameters of the distribution of metric values
        resulting from doing the shuffling of cell cluster assignments.
        - The z-score of the first with respect to the second.
    """
    cluster_pairs = {}
    for cluster_pair in itertools.combinations_with_replacement(
                                                    clusters_list, 2):
        cluster_pairs[cluster_pair] = {}
    return cluster_pairs

def compute_num_cells_per_clones_per_cluster(csv_data_clone_cells):
    """
    Return a multi-level counter with different cell counts per level.

    Each count is stored in a field named "total".
    What is counted on each level:
        1) Total number of cells.
        2) Total number of cells per cluster.
        3) Total number of cells per clone per cluster.
    """
    num_cells = collections.Counter()
    num_cells["total"] = 0
    for cell in csv_data_clone_cells:
        cluster = cell["ident"]
        clone = cell["cloneID"]
        num_cells["total"] += 1
        if(cluster not in num_cells):
            num_cells[cluster] = collections.Counter()
            num_cells[cluster]["total"] = 0
        num_cells[cluster]["total"] += 1
        if(clone not in num_cells[cluster]):
            num_cells[cluster][clone] = collections.Counter()
            num_cells[cluster][clone]["total"] = 0
        num_cells[cluster][clone]["total"] += 1
    return num_cells

def compute_total_num_cells_of_shared_clone_and_cluster_pair(clone
    , cluster_pair, num_cells):
    """
    Return the number of cells in a cluster pair and its shared clone.

    For a clone and a cluster pair,
    if the clone was "shared" by the cluster pair (according to the
    corresponding definition in the Methods section of the paper),
    count the cells that belonged to the clone
    and also were assigned to one of the two clusters.
    If the clone was not shared between the clusters, return 0.
    """
    total_num_cells_of_shared_clone_and_cluster_pair = 0
    # The number of cells of the shared clone of each cluster is added
    for cluster in cluster_pair:
        # If no cells of a clone were assigned to any of the clusters
        if(clone not in num_cells[cluster]
                    or num_cells[cluster][clone]["total"] < 1):
            # Then that clone isn't "shared" by the pair
            return 0
        # If the cluster pair is composed by the same cluster
        if(cluster_pair[0] == cluster_pair[1]):
            # Then the total number of cells of the shared clone
            # is just that of that cluster
            total_num_cells_of_shared_clone_and_cluster_pair = (
                                        num_cells[cluster][clone]["total"])
            break
        else:
            total_num_cells_of_shared_clone_and_cluster_pair += (
                                        num_cells[cluster][clone]["total"])
    # If there were less than 2 cells of a clone
    # that were assigned to any of the clusters, then
    # that clone isn't "shared" between them
    if(total_num_cells_of_shared_clone_and_cluster_pair < 2):
        return 0
    else:
        return total_num_cells_of_shared_clone_and_cluster_pair

def compute_metric_value_single_pair(num_cells, cluster_pair, clones):
    """
    Return the metric value of a cluster pair.

    (According to the definition
    given in the Methods section of the paper).
    """
    metric_value = 0
    # For each clone that had cells assigned to any of the states
    for clone in set(itertools.chain(num_cells[cluster_pair[0]].keys()
                                    , num_cells[cluster_pair[1]].keys())
                    ):
        # Note that in the previous operation, as num_cells stores
        # the total value in a separated field, in the same level that
        # the clones, the "total" field is also included
        if(clone == "total"):
            # So skip it
            continue
        # Only the cells that belonged to shared clones are added
        total_num_cells_of_shared_clone_and_cluster_pair = (
                compute_total_num_cells_of_shared_clone_and_cluster_pair(
                                            clone, cluster_pair, num_cells))
        # Compute and store the relative value with respect to the
        # total number of cells in the (shared) clone
        metric_value += (total_num_cells_of_shared_clone_and_cluster_pair
                                                            / clones[clone])
    return metric_value

def compute_metric_values_all_pairs(num_cells, cluster_pairs, clones):
    """
    Return the metric value of each cluster pair.

    It is stored in a dictionary field, indexed by the cluster pair.
    """
    for cluster_pair in cluster_pairs:
        cluster_pairs[cluster_pair]["metric_value"] = (
                    compute_metric_value_single_pair(
                                num_cells, cluster_pair, clones))
    return cluster_pairs

def random_sample_with_frequency(freq_table):
    """
    Return a random sample of a given distribution.

    The elements are determined by freq_table, which is expected to be
    a collections.Counter() object
    (and hereby store counts of elements).
    """
    elements = list(freq_table.elements())
    total_elements = len(elements)
    sample = tuple(random.sample(elements, k=total_elements))
    return list(sample)

def print_matrix_onto_csv(array_2d, clusters_names_array_order
    , csv_matrix_output_file):
    """
    Write a matrix of values for each cluster pair into a csv file.
    """
    with open(csv_matrix_output_file, 'w', newline='') as csvfile:
        writer = csv.writer(csvfile)
        # Write the column indexes in the first row
        writer.writerow(["ident_name"] + clusters_names_array_order)
        # Write each row, beginning by its corresponding index
        for cluster_name, row in zip(
                                clusters_names_array_order, array_2d):
            writer.writerow([cluster_name] + list(row))
        csvfile.close()

def write_matrix(title, fieldname, cluster_pairs, clusters, clusters_names
    , csv_matrix_output_file):
    """
    Write a matrix of values of each cluster pair into a csv file.

    Also, return the matrix in a 2D list form,
    and a list with the cluster names.
    Parameters:
        title: string that contains the title of the matrix to be
        outputed. It's just for outputing a message in the console.
        fieldname: in the dictionary cluster_pairs, it's the
        name of the field whose values are desired to be written.
        cluster_pairs: dictionary indexed by the cluster pairs,
        among whose attributes should be fieldname.
        clusters: a collections.Counter() object with the
        frequency distribution of the clusters.
        clusters_names: a list that stores, for each clusterID,
        its corresponding name.
        csv_matrix_output_file: string with the file path where
        the matrix is desired to be written.
    """
    # First, transform the data
    # to a structure accepted by the used libraries.

    # Create a 2-D array form of the values in cluster_pairs,
    # and a list that stores the clusters in the order
    # in which they occur in that array (along the columns or rows)
    matrix, clusters_list_array_order = (
                dict_to_array(cluster_pairs, len(clusters)
                                            , fieldname)
            )
    print(title + ":")
    print_2D_float_list(matrix)
    # Create a list that stores the cluster names, in the order
    # in which they occur in the 2D array (along the columns or rows)
    clusters_names_array_order = [
            clusters_names[cluster] for cluster in clusters_list_array_order]
    print_matrix_onto_csv(matrix, clusters_names_array_order
                            , csv_matrix_output_file)

    return matrix, clusters_names_array_order

def compute_distribution_parameters(number_list):
    """Return the mean and standard deviation of a number list."""
    return (float(statistics.mean(number_list))
            , float(statistics.stdev(number_list)))

def compute_distributions(cluster_pairs, num_shufflings
    , csv_data_clone_cells, clusters):
    """
    Return distribution parameters for each cluster pair.

    Do the cell cluster assignment shufflings,
    simulate the resulting assignments, and for each pair of clusters
    compute and store in a list
    the metric values resulting from each shuffling.
    Then compute and store the means and standard deviations
    of the resulting distributions.
    """
    # List that will store the simulated data in each shuffling
    csv_data_clone_cells_local_shuffle = copy.deepcopy(csv_data_clone_cells)
    # Dictionary that will store in lists the metric values
    # obtained over the shufflings for each cluster pair
    cluster_pairs_shufflings = generate_cluster_pairs(clusters.keys())
    for cluster_pair in cluster_pairs_shufflings:
        cluster_pairs_shufflings[cluster_pair]["metric_value"] = []
    # Dictionary, local to each shuffling,
    # that will store the metric values for each cluster pair
    cluster_pairs_local_shuffle = generate_cluster_pairs(clusters.keys())

    for shuffling in range(0, num_shufflings):
        # Perform the random shuffling of cluster assigments
        clusters_assignment = random_sample_with_frequency(clusters)
        # Simulate cell cluster assignments
        for i, csv_data_clone_cell_local in enumerate(
                                        csv_data_clone_cells_local_shuffle):
            csv_data_clone_cell_local["ident"] = clusters_assignment[i]
        # Compute the metric value for each cluster pair
        # observed in the current sample.
        clones = compute_clones_distribution(
                                        csv_data_clone_cells_local_shuffle)
        num_cells = compute_num_cells_per_clones_per_cluster(
                                        csv_data_clone_cells_local_shuffle)
        cluster_pairs_local_shuffle = compute_metric_values_all_pairs(
                            num_cells, cluster_pairs_local_shuffle, clones)
        # Append the current shuffling's results to the lists
        for cluster_pair in cluster_pairs_local_shuffle:
            cluster_pairs_shufflings[
                    cluster_pair]["metric_value"].append(
                                        cluster_pairs_local_shuffle[
                                                cluster_pair]["metric_value"])

    # Compute, for each cluster pair, the parameters
    # of the metric values distributions resulting from the simulations
    for cluster_pair in cluster_pairs_shufflings:
        (cluster_pairs[cluster_pair]["avg_metric_value"]
                , cluster_pairs[cluster_pair]["stdv_metric_value"]) \
                = compute_distribution_parameters(
                                        cluster_pairs_shufflings[
                                                cluster_pair]["metric_value"])
    return cluster_pairs

def compute_z_scores(cluster_pairs):
    """
    Return the z-scores for each pair of clusters.

    The z-scores are of the metric values of the clusters
    (according to the definition
    given in the Methods section of the paper),
    resulting from the data in the dataset,
    with respect to their corresponding distributions of metric values
    resulting from doing the cell cluster assignments shufflings.
    Obs:
        There is a special case when a pair of clusters
        obtains the same metric value in all shufflings
        (and hereby its distribution's stdev=0
        and the z-score is not defined).
        A "N/A" value is assigned to those clusters in this case
        instead of a z-score value.
        In practice, however, this case is unlikely, and even more
        when N is large.
    """
    cluster_pairs_infinite_z_score = []
    for cluster_pair in cluster_pairs:
        deviation = (float(cluster_pairs[cluster_pair]["metric_value"])
                        -cluster_pairs[cluster_pair]["avg_metric_value"])
        if(cluster_pairs[cluster_pair]["stdv_metric_value"] == 0):
            print("Pair {} had a stdev = 0, with a deviation = {}"
                                            .format(cluster_pair, deviation))
            print(cluster_pairs[cluster_pair])
            cluster_pairs_infinite_z_score.append(cluster_pair)
        else:
            cluster_pairs[
                    cluster_pair]["metric_value_z_score"] = (
                                    deviation
                                    / cluster_pairs[
                                            cluster_pair]["stdv_metric_value"]
                            )
    for cluster_pair in cluster_pairs_infinite_z_score:
        cluster_pairs[cluster_pair]["metric_value_z_score"] = "N/A"
    return cluster_pairs

def print_2D_float_list(l):
    """Print a 2D list of floats in a row-wise format."""
    for row in l:
        for col in row:
            print("{:8.1f}".format(col), end=" ")
        print("")

def dict_to_array(cluster_pairs, num_clusters, key_name):
    """
    Return a 2D list with a value for each cluster pair, and their IDs.

    The value for each cluster pair is obtained from the key
    named key_name.
    The 2D list to be returned is conceptually considered to be
    a 2D array, with the rows and columns corresponding to
    the first and second clusters of the cluster pairs,
    and viceversa (to fill the whole matrix).
    The cell value will be the corresponding item stored in the field
    key_name.
    The list of row/column IDs is in insertion order, which is the same
    that the order in which they appear in a cluster pair
    when the latter are iterated over.
    Obs:
        The latter means that the order of the cluster IDs (idents)
        is not maintained, but it depends on
        when it appears within a cluster pair,
        and when that cluster pair appears
        in the iteration of cluster pairs.
    """
    # Ordered Dictionary that will store the indexes in the array
    # corresponding to each of the clusters
    array_indexes = collections.OrderedDict()
    max_array_index = -1
    # Initialize the array to a (num_clusters x num_clusters) shape
    array_2D = np.array([
                                [0.0 for i in range(num_clusters)]
                            for j in range(num_clusters)])
    for cluster_pair in cluster_pairs:
        for cluster in cluster_pair:
            if(cluster not in array_indexes):
                max_array_index += 1
                print("array_indexes[{}] = {}"
                        .format(cluster, max_array_index))
                # As array_indexes is an Ordered Dict, the order in
                # which the indexes were inserted (and the dict keys
                # created) will correspond to the indexes themselves
                array_indexes[cluster] = max_array_index
        # Fill the matrix with the corresponding value
        array_2D[
                array_indexes[cluster_pair[0]]][
                array_indexes[cluster_pair[1]]] = cluster_pairs[cluster_pair][
                                                                    key_name]
        # In both orders (to fill the whole matrix)
        array_2D[
                array_indexes[cluster_pair[1]]][
                array_indexes[cluster_pair[0]]] = cluster_pairs[cluster_pair][
                                                                    key_name]
    # The keys in insertion order of array_indexes are the
    # clusterIDs in the order that they will appear in the new 2D array
    keys_in_insertion_order = [key for key in array_indexes]
    return array_2D, keys_in_insertion_order

def plot_clustermap(title, array_2D, labels_array_order, vmin, vmax, output_dir, add_annot):
    """
    Plot clustermap of the values in array_2D.

    An optional step of hiding the upper triangular matrix
    can be taken.
    """
    # print(title)
    # print_2D_float_list(array_2D)
    mask = np.zeros_like(array_2D)
    # Set the next assigment to False for plotting
    # the whole matrix, and to True for only the lower triangular matrix
    # with the diagonal
    mask[np.triu_indices_from(mask)] = False
    mask[np.diag_indices_from(mask)] = False
    with sns.axes_style("white"):
        ax = sns.clustermap(array_2D
                    , xticklabels = labels_array_order
                    , yticklabels = labels_array_order
                    , mask=mask, vmin=vmin, vmax=vmax
                    , annot=add_annot
                    # , square=True
                    , cmap="Blues")
        plt.setp(ax.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)
    # save heatmap
    plt.savefig(output_dir, dpi=300, bbox_inches="tight", format="png")
    plt.show()

def output_lineage_coupling_computations(
    cluster_pairs, clusters, clusters_names
    , csv_lineage_coupling_scores_matrix_output_file, vmin, vmax, output_dir, add_annot=False):
    """
    Output computations related to clusters' lineage couplings.

    The computations of the lineage coupling scores,
    outputed in the form of:
        - a matrix onto a csv
        - and a clustermap in a plot
    """
    # First, transform data to a structure accepted by seaborn methods.

    # The 2D array form of the z-scores,
    # and a list that stores the order of the clusters
    # in which they occur in that array (along the columns or rows)
    z_scores_array, clusters_list_array_order = (
                dict_to_array(cluster_pairs, len(clusters)
                        , "metric_value_z_score")
            )
    print("Lineage Coupling Matrix:")
    print_2D_float_list(z_scores_array)
    clusters_names_array_order = [
            clusters_names[cluster] for cluster in clusters_list_array_order]
    print_matrix_onto_csv(z_scores_array, clusters_names_array_order
                , csv_lineage_coupling_scores_matrix_output_file)
    plot_clustermap("Lineage Coupling Scores", z_scores_array
                                    , clusters_names_array_order, vmin, vmax, output_dir, add_annot)

# construct euclidean distance
n=10
def compute_distances(X):
  # n is the number of points

 return squareform(pdist(X, 'euclidean'))

# compute affinity matrix
def compute_affinity_matrix(D, kernel_type, sigma=None, k=None):
  W = np.zeros((n,n), dtype = 'float64')
  if kernel_type == 'gaussian':
        sigma > 0
        W= np.exp(-D**2/sigma**2)
  else:
        kernel_type == 'adaptive'
        sigma = np.zeros((n,1), dtype = 'float64')
        for m in range(0,n):
          # sort distance in ascending order of each row in D (distance matrix)
           sortD=sorted(D[m,:],reverse=False)
          # construct sigma where sigma of i is the distance from i to its k nearest neighbor
           sigma[m]=sortD[k]
        for i in range(0,n):
          for j in range (0,n):
          # compute adapative gaussian kernel
            W[i][j]=0.5 *(np.exp(-D[i][j]**2/sigma[i]**2)+np.exp(-D[i][j]**2/sigma[j]**2))

  return W

def diff_map_info(W):
# compute D (diagonal matrix of row sums)
 Dia=np.diag(np.sum(W, axis=1))
# create symmetrix matrix
# diffusion matrix
 Ms=la.inv(Dia)**(1/2)@ W @la.inv(Dia)**(1/2)
# compute eigenpairs of Ms and order eigenvector in descending order
 eigval, eigvec = la.eigh(Ms)
 idx=eigval.argsort()[::-1]
 eigval=eigval[idx]
 eigvec=eigvec[:,idx]
# compute eigenvectors and eigenvalues of M
 diff_vec1=la.inv(Dia**(1/2))@ eigvec
 diff_vec=(diff_vec1/np.linalg.norm(diff_vec1,axis=0))
 diff_eig=eigval[1:]
 diff_vec=diff_vec[:,1:]
 return diff_vec,diff_eig

def get_diff_map(diff_vec,diff_eig,t):
  # eigenvalue in diffusion map
  eigvaluet=diff_eig**t
  diff_map=diff_vec @ np.diag(eigvaluet)
  return diff_map



